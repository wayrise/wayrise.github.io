<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
    content="Data Scaling Laws in Imitation Learning for Robotic Manipulation">
  <meta name="keywords" content="Data Scaling Laws, Imitation Learning, Robotic Manipulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>
    DexGarmentLab
  </title>

  <!-- Thumbnail for social media sharing -->
  <meta property="og:image" content="media/images/pku_logo.png">

  <!-- Favicon -->
  <link rel="icon" href="media/images/pku_logo.png" type="image/jpeg">

  <script>
    window.dataLayer = window.dataLayer || [];
  </script>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <link rel="stylesheet" href="./static/source_serif_4.css">
  <link rel="stylesheet" href="./static/source_sans_3.css">  
  <link rel="stylesheet" href="./static/academicons.min.css">
  <link rel="stylesheet" href="./static/fontawesome/css/fontawesome.css">
  <link rel="stylesheet" href="./static/fontawesome/css/brands.css">
  <link rel="stylesheet" href="./static/fontawesome/css/light.css">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.3/css/bulma.min.css">
  <script type="module" src="https://unpkg.com/ionicons@5.5.2/dist/ionicons/ionicons.esm.js"></script>
</head>
<!-- <body onload="updateInTheWild();updateBimanual();"> -->

<section class="hero">
  <div class="hero-body" style="padding-top: 1.5%;">
    <div class="container is-fullhd">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <img src="media/images/dexgarmentlab-main.jpeg" alt="ReKep" style="display: block; margin: 0 auto; width: 70%">
          <img src="media/images/dexgarmentlab-title.jpeg" alt="ReKep" style="display: block; margin: 0 auto; width: 90%">
          <br>
          <!-- <h1 class="title is-1 publication-title" style="color:#775295;">in Imitation Learning for Robotic Manipulation</h1> -->
          <div class="is-size-4 accepted" style="color:#ad1010">
            Under Review
          </div>
          <!-- <div class="is-size-4 award" style="color:#ad1010">
            Best Paper Award at Workshop on X-Embodiment Robot Learning, CoRL 2024
          </div> -->
          <br>
          <div class="is-size-5 publication-authors">
            <span class="author-block" style="color:#ad1010">
              <a target="_blank" href="https://github.com/wayrise">Yuran Wang</a><sup>1*</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://warshallrho.github.io/">Ruihai Wu</a><sup>1*</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://yuechen0614.github.io/homepage/">Yue Chen</a><sup>1*</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://github.com/Jr-kelly">Jiarui Wang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://github.com/SCreatorX">Jiaqi Liang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://alwaysleepy.github.io/">Ziyu Zhu</a><sup>1</sup>,
            </span>
            <br>
            <span class="author-block">
              <a target="_blank" href="https://geng-haoran.github.io/">Haoran Geng</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://zsdonghao.github.io/">Hao Dong</a><sup>1</sup>
            </span>
          </div>
          <div class="is-size-5 affiliation">
            <sup>1</sup>Peking University,
            <sup>2</sup>University of California, Berkeley
          </div>
          <br>
          <div class="affiliation-note">
            <sup>*</sup> indicates equal contributions
          </div>
          <div class="button-container">
            <a href="https://wayrise.github.io/DexGarmentLab/" target="_blank" class="button" style="background-color: #f2f2f2;border: transparent;"><i class="ai ai-arxiv"></i>&emsp14;arXiv</a>
            <!-- <a href="https://wayrise.github.io/DexGarmentLab/" target="_blank" class="button" style="background-color: #f2f2f2;border: transparent;"><i class="ai ai-arxiv"></i>&emsp14;Page</a> -->
            <!-- <a href="https://youtu.be/2S8YhBdLdww" target="_blank" class="button"><i class="fa-light fa-film"></i>&emsp14;Video</a> -->
            <!-- <a href="https://x.com/wenlong_huang/status/1829135436717142319" target="_blank" class="button"><i class="fa-brands fa-x-twitter"></i>&emsp14;tl;dr</a> -->
            <a href="https://github.com/wayrise/DexGarmentLab" target="_blank" class="button" style="background-color: #f2f2f2;border: transparent;"><i class="fa-light fa-code"></i>&emsp14;Code</a>
            <a href="https://huggingface.co/datasets/wayrise/DexGarmentLab/tree/main" target="_blank" class="button" style="background-color: #f2f2f2;border: transparent;"><i class="fa-light fa-folder"></i>&emsp14;Data</a>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-widescreen">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-vcentered  is-centered">
          <video id="teaser" autoplay muted loop controls height="100%" width="100%">
            <source src="media/videos/teaser_nomask.mp4"
                    type="video/mp4">
          </video>
          </br>
        </div>
        <br>
        <h2 class="subtitle has-text-centered">
        We show that with proper data scaling, a single-task policy can generalize well to <strong>any new environment</strong> and 
        <strong>any new object</strong> within the same category. Remarkably, the robot can even be deployed zero-shot in a hot pot restaurant üç≤!
        </h2>
      </div>
    </div>
  </div> -->

<br>
<div class="container is-max-widescreen">
<div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <h2 class="title is-3" style="margin-bottom: 0.5em; color:#ad1010">Abstract</h2>
    <div class="content has-text-justified">
      <p>
          Garment manipulation is a critical challenge due to the diversity in garment categories, geometries, and deformations. 
          Despite this, humans can effortlessly handle garments, thanks to the dexterity of our hands. 
          However, existing research in the field has struggled to replicate this level of dexterity, primarily hindered by the lack of realistic simulations of dexterous garment manipulation.
          Therefore, we propose <b>DexGarmentLab</b>, the first environment specifically designed for dexterous (especially bimanual) garment manipulation, 
          which features large-scale high-quality 3D assets for 15 task scenarios, and refines simulation techniques tailored for garment modeling to reduce the sim-to-real gap. 
          Previous data collection typically relies on teleoperation or training expert reinforcement learning (RL) policies, which are labor-intensive and inefficient.
          In this paper, we leverage <b>garment structural correspondence</b> to <b>automatically</b> generate a dataset with diverse trajectories using only a single expert demonstration, significantly reducing manual intervention.
          However, even extensive demonstrations cannot cover the infinite states of garments, which necessitates the exploration of new algorithms. 
          To improve generalization across diverse garment shapes and deformations, we propose a <b>Hierarchical gArment-manipuLation pOlicy (HALO)</b>.
          It first identifies transferable affordance points to accurately locate the manipulation area, then generates generalizable trajectories to complete the task.
          Through extensive experiments and detailed analysis of our method and baseline, we demonstrate that HALO consistently outperforms existing methods, 
          successfully generalizing to previously unseen instances even with significant variations in shape and deformation where others fail.
        </p>
    </div>
  </div>
</div>

<br>
<br>
<div class="rows is-centered has-text-centered">
  <h2 class="title is-3" style="margin-bottom: 0.5em; color:#ad1010">Overview</h2>
  <img src="media/images/Teaser.jpg" class="method-image" />
  <!-- <div class="columns is-vcentered  is-centered">
    <video id="teaser" autoplay muted loop  height="90%" width="90%">
      <source src="media/videos/curve.mp4"
              type="video/mp4">
    </video>
  </div> -->
  <br>
  <br>
  <p class="content has-text-justified" style="margin-bottom: 0.6em">
    <b>DexGarmentLab</b> includes three major components:
    <br>
    &nbsp;&nbsp;- <b>Environment</b>: We propose <u>Dexterous Garment Manipulation Environment</u> with 15 different task scenes (especially for bimanual coordination) based on 2500+ garments.
    <br>
    &nbsp;&nbsp;- <b>Automated Data Collection</b>: Because of the same structure of category-level garment, category-level generalization is accessible, which empowers our proposed <u>Automated Data Collection Pipeline</u> to handle different position, deformation and shapes of garment with task config (including grasp position and task sequence) and grasp hand pose provided by single expert demonstration.
    <br>
    &nbsp;&nbsp;- <b>Generalizable Policy</b>: With diverse collected demonstration data, we introduce <u> <b>H</b>ierarchical g<b>A</b>rment manipu<b>L</b>ation p<b>O</b>licy (<b>HALO</b>) </u>, combining affordance points and trajectories to generalize across different attributes in different tasks.
  </p>
</div>

<br>
<br>
<div class="rows is-centered has-text-centered">
  <h2 class="title is-3" style="margin-bottom: 0.5em; color:#ad1010">Simulation Environment</h2>
  <img src="media/images/Benchmark.jpg" class="method-image" />
  <!-- <div class="columns is-vcentered  is-centered">
    <video id="teaser" autoplay muted loop  height="90%" width="90%">
      <source src="media/videos/curve.mp4"
              type="video/mp4">
    </video>
  </div> -->
  <br>
  <br>
  <p class="content has-text-justified" style="margin-bottom: 0.6em">

  </p>
</div>

<br>
<br>
<br>
<br>
<div class="rows is-centered has-text-centered">
  <h2 class="title is-3" style="margin-bottom: 0.5em; color:#ad1010">Evaluation Videos</h2>
  <p class="content" style="text-align: left">
    Building upon power-law data scaling laws, we propose an efficient data collection strategy. By 
    collecting data from numerous environments (e.g., 32 environments), each featuring a unique manipulation 
    object and 50 demonstrations, we can train a policy that 
    generalizes effectively‚Äîachieving a 90% success rate‚Äîto any new environment and object. Below, we 
    present sample rollouts from <strong>8 unseen testing environments</strong> investigated in the paper.
  </p>

  <div class="container block is-centered">
    <div class="columns is-centered">
      <div class="column is-2" style="width: 22%;">
          <div class="select" style="display: block;">
              <select id="env-selection" style="width: 100%;" onchange="SelectTestVideo()">
                  <option value="env_1">Env 1 (Iron Cabinet)</option>
                  <option value="env_2">Env 2 (Wooden Podium)</option>
                  <option value="env_3">Env 3 (Black Workbench)</option>
                  <option value="env_4">Env 4 (Two Chairs)</option>
                  <option value="env_5">Env 5 (Cart)</option>
                  <option value="env_6">Env 6 (Workstaion)</option>
                  <option value="env_7">Env 7 (Meeting Room 1)</option>
                  <option value="env_8">Env 8 (Meeting Room 2)</option>
              </select>
          </div>
      </div>
      <div class="column is-2" style="width: 20%;">
          <div class="select" style="display: block;">
              <select id="task-selection" style="width: 100%;" onchange="SelectTestVideo()">
                <option value="pour_water">Pour Water</option>
                <option value="pick_place_mouse">Mouse Arrangement</option>
                <option value="fold_towel">Fold Towels</option>
                <option value="unplug">Unplug Charge</option>
              </select>
          </div>
      </div>
      <div class="column is-2" style="width: 17%;">
          <div class="select" style="display: block;"  onchange="SelectTestVideo()">
              <select id="object-selection" style="width: 100%;">
                <option value="1">Unseen Object 1</option>
                <option value="2">Unseen Object 2</option>
              </select>
          </div>
      </div>
      <div class="column is-2">
        <button class="button is-info is-outlined" id="shuffle-video" style="width: 100%; color: #ad1010; border-color: #ad1010;">
            <span class="icon">
                <ion-icon name="shuffle-outline" role="img" class="md hydrated"></ion-icon>
            </span>
            <span>Shuffle</span>
        </button>
    </div>
    </div>
  </div>

  <div class="columns is-centered">
    <div class="column has-text-centered">
      <p style="text-align:center;">
        <video id="test-video" width="80%" height="100%" controls autoplay loop muted>
          <source src="media/videos/test-wild-videos/env_1/pour_water/1.mp4" type="video/mp4">
        </video>
      </p>
    </div>
  </div>
</div>

<br>
<br>
<br>
<section class="section" style="padding: 0;">
  <div class="container is-max-widescreen">

    <div class="rows">

      <div class="container has-text-centered">
        <h2 class="title is-3" style="margin-bottom: 0.5em; color:#ad1010">More In-The-Wild Environments</h2>
        <p class="content" style="margin-bottom: 1.5em; text-align: left;">
          We deployed the robot in various in-the-wild environments‚Äîincluding hot pot restaurants üç≤, caf√©s ‚òï, elevators üõó, fountains ‚õ≤, 
          and other locations where data had not been previously collected. We found that the policy generalized surprisingly well!
        </p>
      </div>

      <!-- <p class="content has-text-centered">
        <span style="letter-spacing: 0.07em;">V<span style="font-variant: small-caps; font-size: 1.2em;">i</span>L<span style="font-variant: small-caps; font-size: 1.2em;">a</span></span> effectively utilizes visual feedback in an intuitive and natural way, enabling robust closed-loop planning in dynamic environments.
      </p> -->

      <div class="columns">
        <div class="column has-text-centered">
          <video id="dist1"
            controls
            muted
            autoplay
            loop controlsList="nodownload"
            width="99%">
            <source src="media/videos/wild-autonomous-skills/1.mp4" 
            type="video/mp4">
          </video>
        </div>

        <div class="column has-text-centered">
          <video id="dist2"
            controls
            muted
            autoplay
            loop controlsList="nodownload"
            width="99%">
            <source src="media/videos/wild-autonomous-skills/2.mp4" 
            type="video/mp4">
          </video>
        </div>
      </div>

      <div class="columns">
        <div class="column has-text-centered">
          <video id="dist1"
            controls
            muted
            autoplay
            loop controlsList="nodownload"
            width="99%">
            <source src="media/videos/wild-autonomous-skills/3.mp4" 
            type="video/mp4">
          </video>
        </div>

        <div class="column has-text-centered">
          <video id="dist2"
            controls
            muted
            autoplay
            loop controlsList="nodownload"
            width="99%">
            <source src="media/videos/wild-autonomous-skills/4.mp4" 
            type="video/mp4">
          </video>
        </div>
      </div>

      <div class="columns">
        <div class="column has-text-centered">
          <video id="dist1"
            controls
            muted
            autoplay
            loop controlsList="nodownload"
            width="99%">
            <source src="media/videos/wild-autonomous-skills/6.mp4" 
            type="video/mp4">
          </video>
        </div>

        <div class="column has-text-centered">
          <video id="dist2"
            controls
            muted
            autoplay
            loop controlsList="nodownload"
            width="99%">
            <source src="media/videos/wild-autonomous-skills/5.mp4" 
            type="video/mp4">
          </video>
        </div>
      </div>

      <div class="columns">
        <div class="column has-text-centered">
          <video id="dist1"
            controls
            muted
            autoplay
            loop controlsList="nodownload"
            width="99%">
            <source src="media/videos/wild-autonomous-skills/7.mp4" 
            type="video/mp4">
          </video>
        </div>

        <div class="column has-text-centered">
          <video id="dist2"
            controls
            muted
            autoplay
            loop controlsList="nodownload"
            width="99%">
            <source src="media/videos/wild-autonomous-skills/8.mp4" 
            type="video/mp4">
          </video>
        </div>
      </div>
  </div>
</section>

<br>
<br>
<br>
<div class="rows is-centered has-text-centered">
  <h2 class="title is-3" style="color:#ad1010;">Hardware Setup</h2>
  <img src="media/images/hardware-setups.jpg" class="method-image" />
</div>

<br>
<br>
<br>
<h2 class="title is-3 is-centered has-text-centered" style="color:#ad1010;">Acknowledgments</h2>
<p>
  The robot hardware is partially supported by Tsinghua ISR Lab. We would like to express our gratitude to Cheng Chi and Chuer Pan for their invaluable advice on UMI. We are also thankful to Linkai Wang for his assistance in setting up the movable platform. Additionally, we appreciate the thoughtful discussions and feedback provided by Tong Zhang, Ruiqian Nai, Geng Chen, Weijun Dong, Shengjie Wang, and Renhao Wang.
</p>

<br>
<br>
<h2 class="title is-3 is-centered has-text-centered" style="color:#ad1010;">BibTeX</h2>
<p class="bibtex">
  @article{lin2024data, <br>
  &nbsp;&nbsp;title={Data scaling laws in imitation learning for robotic manipulation}, <br>
  &nbsp;&nbsp;author={Lin, Fanqi and Hu, Yingdong and Sheng, Pingyue and Wen, Chuan and You, Jiacheng and Gao, Yang}, <br>
  &nbsp;&nbsp;journal={arXiv preprint arXiv:2410.18647}, <br>
  &nbsp;&nbsp;year={2024} <br>
  }
</p>

</section>
</div>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://nerfies.github.io">Nerfies</a>
            and <a href="https://rekep-robot.github.io/">ReKep</a>. 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<script>
  document.getElementById('shuffle-video').addEventListener('click', function() {
    var envSelect = document.getElementById('env-selection');
    var taskSelect = document.getElementById('task-selection');
    var objectSelect = document.getElementById('object-selection');
    randomizeSelect(envSelect);
    randomizeSelect(taskSelect);
    randomizeSelect(objectSelect);
    console.log("Shuffle", envSelect.value, taskSelect.value, objectSelect.value)
    SelectTestVideo();
  });

  function randomizeSelect(selectElement) {
    var options = selectElement.options;
    random_move = Math.random();
    var randomIndex = Math.floor(Math.random() * options.length);
    selectElement.selectedIndex = randomIndex;
  }

  function SelectTestVideo() {
    var env_id = document.getElementById("env-selection").value;
    var task_name = document.getElementById("task-selection").value;
    var object_id = document.getElementById("object-selection").value;

    console.log("SelectTestVideo", env_id, task_name, object_id)
    var video = document.getElementById("test-video");
    video.src = "media/videos/test-wild-videos/" + env_id + "/" + 
                task_name + "/" + object_id + ".mp4";
    video.play();
  }
</script>

<style>
  .button.is-info.is-outlined:focus,
  .button.is-info.is-outlined:active {
    background-color: transparent;
    border-color: #ad1010;
    color: #ad1010;
    box-shadow: none;
  }

  .button.is-info.is-outlined:hover {
    background-color: #ad1010;
    color: #fff;
  }

  .button.is-info.is-outlined:hover .icon ion-icon,
  .button.is-info.is-outlined:hover span {
    color: #fff;
  }
</style>

</body>
</html>